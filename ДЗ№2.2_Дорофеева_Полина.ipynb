{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ],
      "metadata": {
        "id": "1xVbvaj_phyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ],
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ],
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ],
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def punctuation_tokenization(text):\n",
        "    \"\"\"Токенизация по пробелам и знакам препинания\"\"\"\n",
        "    # Создаем шаблон для разделения по пробелам и знакам препинания\n",
        "    tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "W1QCaw6cqDnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ],
      "metadata": {
        "id": "GThvPcovqgO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Hello, world! This is a test.\"\n",
        "print(word_tokenize(text))\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "id": "14BIv33iqrkL",
        "outputId": "6f28fc6d-cdc5-4113-83b5-e846d1513568",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '.']\n",
            "['Hello, world!', 'This is a test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ],
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Hello, world! This is a test.\")\n",
        "[t.text for t in doc]"
      ],
      "metadata": {
        "id": "B0NQg-VfuFW_",
        "outputId": "3ad1a9da-f226-46e1-ecdc-4d30533c3398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ],
      "metadata": {
        "id": "WmyJfB9wuKkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvUmk94MhrL8",
        "outputId": "41c6b3b3-f77f-4583-c6d3-c84e56b762dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ТОКЕНИЗАЦИЯ ТЕКСТОВ РАЗНЫМИ МЕТОДАМИ\n",
            "============================================================\n",
            "\n",
            "Текст 1: Hello, world! This is a test.\n",
            "--------------------------------------------------\n",
            "1. Punctuation Tokenization: ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '.']\n",
            "2. NLTK Tokenization: ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '.']\n",
            "3. Spacy Tokenization: ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '.']\n",
            "Кол-во токенов: 9 | 9 | 9\n",
            "\n",
            "Текст 2: Natural language processing (NLP) is amazing!\n",
            "--------------------------------------------------\n",
            "1. Punctuation Tokenization: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'amazing', '!']\n",
            "2. NLTK Tokenization: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'amazing', '!']\n",
            "3. Spacy Tokenization: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'amazing', '!']\n",
            "Кол-во токенов: 9 | 9 | 9\n",
            "\n",
            "Текст 3: I love Python programming. It's so powerful!\n",
            "--------------------------------------------------\n",
            "1. Punctuation Tokenization: ['I', 'love', 'Python', 'programming', '.', 'It', \"'\", 's', 'so', 'powerful', '!']\n",
            "2. NLTK Tokenization: ['I', 'love', 'Python', 'programming', '.', 'It', \"'s\", 'so', 'powerful', '!']\n",
            "3. Spacy Tokenization: ['I', 'love', 'Python', 'programming', '.', 'It', \"'s\", 'so', 'powerful', '!']\n",
            "Кол-во токенов: 11 | 10 | 10\n",
            "\n",
            "Текст 4: How are you doing today? Let's meet tomorrow.\n",
            "--------------------------------------------------\n",
            "1. Punctuation Tokenization: ['How', 'are', 'you', 'doing', 'today', '?', 'Let', \"'\", 's', 'meet', 'tomorrow', '.']\n",
            "2. NLTK Tokenization: ['How', 'are', 'you', 'doing', 'today', '?', 'Let', \"'s\", 'meet', 'tomorrow', '.']\n",
            "3. Spacy Tokenization: ['How', 'are', 'you', 'doing', 'today', '?', 'Let', \"'s\", 'meet', 'tomorrow', '.']\n",
            "Кол-во токенов: 12 | 11 | 11\n",
            "\n",
            "Текст 5: The quick brown fox jumps over the lazy dog.\n",
            "--------------------------------------------------\n",
            "1. Punctuation Tokenization: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "2. NLTK Tokenization: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "3. Spacy Tokenization: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "Кол-во токенов: 10 | 10 | 10\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "# Предварительная загрузка необходимых ресурсов\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Загрузка модели spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# 1. Функция для токенизации по пробелам и знакам препинания\n",
        "def punctuation_tokenization(text):\n",
        "    \"\"\"Токенизация по пробелам и знакам препинания\"\"\"\n",
        "    tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
        "    return tokens\n",
        "\n",
        "# 2. Функция для токенизации текста с помощью NLTK\n",
        "def nltk_tokenization(text):\n",
        "    \"\"\"Токенизация с помощью NLTK\"\"\"\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# 3. Функция для токенизации текста с помощью Spacy\n",
        "def spacy_tokenization(text):\n",
        "    \"\"\"Токенизация с помощью Spacy\"\"\"\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc]\n",
        "    return tokens\n",
        "\n",
        "# Список текстов для обработки\n",
        "texts = [\n",
        "    \"Hello, world! This is a test.\",\n",
        "    \"Natural language processing (NLP) is amazing!\",\n",
        "    \"I love Python programming. It's so powerful!\",\n",
        "    \"How are you doing today? Let's meet tomorrow.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\"\n",
        "]\n",
        "\n",
        "# Применяем каждую функцию к каждому тексту с помощью цикла for\n",
        "print(\"ТОКЕНИЗАЦИЯ ТЕКСТОВ РАЗНЫМИ МЕТОДАМИ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, text in enumerate(texts, 1):\n",
        "    print(f\"\\nТекст {i}: {text}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Применяем функцию punctuation_tokenization\n",
        "    tokens1 = punctuation_tokenization(text)\n",
        "    print(f\"1. Punctuation Tokenization: {tokens1}\")\n",
        "\n",
        "    # Применяем функцию nltk_tokenization\n",
        "    tokens2 = nltk_tokenization(text)\n",
        "    print(f\"2. NLTK Tokenization: {tokens2}\")\n",
        "\n",
        "    # Применяем функцию spacy_tokenization\n",
        "    tokens3 = spacy_tokenization(text)\n",
        "    print(f\"3. Spacy Tokenization: {tokens3}\")\n",
        "\n",
        "    print(f\"Кол-во токенов: {len(tokens1)} | {len(tokens2)} | {len(tokens3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ],
      "metadata": {
        "id": "Mwe1Co6MvibX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)**\n",
        "\n",
        "Простое разделение текста по пробелам и знакам препинания часто оказывается недостаточным для современных задач обработки естественного языка из-за сложности языковых структур и многозначности языковых единиц. Данный подход не учитывает контекстную зависимость, морфологические особенности языков и семантическую целостность составных выражений.\n",
        "\n",
        "**Конкретные примеры проблем:**\n",
        "В английском языке контракции типа \"I'm\" при разделении на [\"I\", \"'m\"] теряют связь с исходной формой глагола \"am\", что затрудняет семантический анализ. Аналогично, отрицательная форма \"don't\" при разбиении на [\"do\", \"n't\"] создает искусственное разделение, не соответствующее грамматической реальности.\n",
        "Составные термины и идиоматические выражения, такие как \"state-of-the-art\" или \"ad hoc\", при разделении на отдельные компоненты теряют свое специализированное значение. Технические термины и профессиональная лексика требуют целостной обработки для сохранения смысловой точности.\n",
        "\n",
        "Многозначность пунктуации создает **дополнительные сложности**: дефис может быть как разделителем, так и частью слова, апостроф выполняет различные функции в притяжательных формах и сокращениях, что требует контекстно-зависимого анализа.\n",
        "\n",
        "2. **Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)**\n",
        "\n",
        "Данная фраза содержит 13 токенов при токенизации с использованием стандартного токенизатора OpenAI, который применяется в моделях GPT. Для получения этого значения можно воспользоваться официальным токенизатором через API OpenAI или библиотекой tiktoken, специально разработанной для работы с токенизацией в моделях семейства GPT.\n",
        "Альтернативно, можно использовать онлайн-инструменты типа \"OpenAI Tokenizer\", которые визуализируют процесс разбиения текста на токены и показывают точное их количество. Важно отметить, что количество токенов может незначительно варьироваться в зависимости от конкретной версии токенизатора и модели, но для современных реализаций GPT указанное значение остается стабильным.\n",
        "\n",
        "\n",
        "3. **Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)**\n",
        "\n",
        "Алгоритм BPE представляет собой компромиссный подход между символьной и словной токенизацией, основанный на статистическом анализе частотности сочетаний символов.\n",
        "\n",
        "**Его работа состоит из нескольких этапов:**\n",
        "На первом этапе исходный текст разбивается на базовые символы или байты, создавая начальный словарь, состоящий из элементарных единиц. Каждое слово представляется как последовательность этих базовых элементов с специальным маркером конца слова.\n",
        "\n",
        "На втором этапе осуществляется итеративный процесс статистического анализа: система подсчитывает частоту всех пар соседних токенов в обрабатываемом корпусе текстов. На каждой итерации выбирается наиболее частая пара токенов для слияния в новый составной токен.\n",
        "\n",
        "Процесс повторяется заданное количество раз или до достижения целевого размера словаря. На каждой итерации словарь пополняется новыми составными токенами, которые образуются путем слияния наиболее частотных пар из предыдущего шага.\n",
        "Ключевые преимущества BPE включают способность обрабатывать редкие и неизвестные слова через их разложение на известные подсловные компоненты, баланс между размером словаря и эффективностью представления, а также адаптивность к различным языкам и доменам. Этот алгоритм успешно применяется в современных языковых моделях, обеспечивая эффективное представление текстовой информации."
      ],
      "metadata": {
        "id": "mgE2bQFXv0MG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}